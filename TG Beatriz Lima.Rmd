--- 
title: "Ciência de dados na Engenharia Ambiental e Urbana: abordagem introdutória com aplicações"
author: "Beatriz Morais Lima e Wallace Gusmão Ferreira"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
# url: your book url like https://bookdown.org/yihui/bookdown
# cover-image: path to the social sharing image like images/cover.jpg
description: |
  This is a minimal example of using the bookdown package to write a book.
  The HTML output format for this example is bookdown::gitbook,
  set in the _output.yml file.
link-citations: yes
github-repo: rstudio/bookdown-demo
---

# Sobre {-}

Livro relacionado a técnicas de programação e modelagem de temas específicos da **Engenharia Ambiental e Urbana**. Será realizado um conjunto de materiais teóricos, equacionamentos, fluxo de dados e principais análises numéricas e gráficas. Além da elaboração de estudos de casos e tutoriais didáticos relacionados a esses tópicos da Engenharia Ambiental e Urbana.



# R - Introdução e Comandos Básicos

R é uma linguagem e ambiente para computação estatística e gráficos. Essa linguagem de programação fornece uma grande variedade de técnicas estatísticas (modelagem linear e não-linear, testes estatísticos clássicos, análise de séries temporais, etc) e gráficas (The R Foundation, 2022).


## Definindo um diretório de trabalho

O **diretório de trabalho** é o local onde a base de dados de um projeto está salva. Para verificar qual é o diretório de trabalho atual, basta utilizar a função ` getwd() `.

```
getwd()
```

Para definir um novo diretório de trabalho, basta utilizar a função `setwd()` e inserir como argumento o caminho desejado, conforme exemplo abaixo.

```
setwd("Home/Beatriz/documentos/Data Science")
```
## Vetores

Vetores permitem que sejam armazenados conjuntos de valores sob um mesmo nome. Serão apresentadas três maneiras de criar vetores abaixo.

  1. Utilizando a função `c(v1, v2, v3,..., vi)` que concatena os valores presentes no argumento e cria um vetor;
  
  ```
  > Valores <- c(1,2,3,4,5)
  ```
  
  2. Utilizando o comando `seq(from,to, by, length)` que gera sequências números de um valor inicial (from) até um valor final (to) com um incremento (by) e um comprimento (length).
  
  ```
  Exemplo 1: seq (from, to, by)
  > Valores <- seq(1,6,by=1)
  [1] 1.0 3.0 3.0 4.0 5.0 6.0
  ```
  ```
  Exemplo 2: seq (from, to, length)
  > Valores <- seq(2,10,length=5)
  [1] 2 4 6 8 10
  ```
  ```
  Exemplo 3: seq (from, to, length)
  > Valores <- seq(from=2,by=10,length=6)
  [1] 2 12 22 32 42 52 
  ```
  
  3. Utilizando o comando `from:to` que cria a sequência de um valor inicial (from) até um final (to) com o incremento de by=1.
  ```
  Exemplo: 
  > Valores <- 1:10
  [1] 1 2 3 4 5 6 7 8 9 10
  ```
  
## Base de dados

Para realizar a **leitura** de uma base de dados, basta utilizar a função `read() `. Exemplos de **formatos de arquivo** são .csv, .txt, .xlsx, entre outros. A forma mais comum de disponibilização de dados é o formato `.csv` e a leitura dessa base de dados pode ser realizada conforme demonstrado abaixo. 

```dados <- read.csv("Data Science/dados.csv")
```

Note que o caminho escrito está pela metade, pois o diretório de trabalho já foi definido. Dessa forma, não é necessário escrever o caminho inteiro do arquivo "dados.csv", por já estar localizado no diretório.

Para **exploração dos dados**, inicialmente algumas funções são úteis: 

  1. `View()` para visualização dos dados .csv em uma tabela;
  2. `Names()` para exibição dos nomes das variáveis presentes na base de dados;
  3. `Summary()` para a realização de um sumário de estatísticas para as variáveis numéricas da base de dados (mínimo, máximo, mediana, média, 1º quartil, 3º quartil e número de NAs).
  
  
## Seleção de variáveis

Para realizar a análise dos dados, é necessário selecionar variáveis do banco de dados e filtrar seus valores. Ou seja, é necessário criar **subconjuntos**. Em R, estes podem ser criados com o uso de colchetes: `dados[observações,variáveis]`.
Por exemplo, para um conjunto de dados com numerações para cada uma das observações, se desejamos selecionar as 4 primeiras observações, basta realizar o comando `dados[1:4,]`.

## R packages
Um *R package* é um conjunto de funções que extendem a capacidade da base R. Na análise exploratória de dados, um pacote importante para a organização dos dados é o tidyverse(); para visualizações, o ggplot2(); e, para gráficos de correlação, corrplot(). Para o instalar, basta escrever a seguinte linha de código, inserindo no argumento o nome do pacote:

```
install.packages("tidyverse")
```

Após instalar um pacote, é necessário carregá-lo para poder utilizar todas as suas funcionalidades. Portanto, basta utilizar a função `library()` para instalar, inserindo o nome do pacote como argumento. Nesse caso, não utiliza-se aspas no argumento.

<!--chapter:end:index.Rmd-->

# Análise Exploratória de Dados 

A **Análise Exploratória de Dados** (EDA) é uma forma de utilizar ferramentas gráficas e estatísticas apropriadas da linguagem R na exploração de dados. A partir da visualização, transformação e modelagem de dados, essa análise explora os dados de forma sistemática (WICKHMAN & GROLEMUND, 2017).

Para aplicar a EDA, de acordo com Wickhman & Grolemund (2017), basta inicialmente gerar questões sobre os dados, utilizar as funcionalidades do R para encontrar respostas e utilizar o que foi aprendido para gerar novas questões. Portanto, essa análise é um ciclo iterativo.

## População e amostra

A **População** é um conjunto de observações relacionadas a indivíduos com uma característica em comum. Já a **Amostra** é um subconjunto da população, com uma parte das observações relacionadas à população. A partir da amostra realizam-se inferências sobre as características da população. 

É importante que a amostra seja representativa para que os resultados não sejam deturpados.


## Sumarização numérica de dados

Resumos numéricos de dados são fundamentais para realizar inferências estatísticas, porque, de acordo com Montgomery & Runger (2021), "*permitem ao engenheiro focar nas características importantes dos dados ou ter discernimento acerca do tipo de modelo que deveria ser usado na solução do problema*". Por isso, torna-se útil descrever numericamente, a partir de medidas de posição, variabilidade e forma, características dos dados.

### Medidas de Posição

#### Média

A média é a divisão da soma de todos os valores da série pelo número de obervações n.


  $\bar{x}=\frac{x_{1}+x_{2}+x_{3}...x_{n}}{n}=\frac{\sum x_n}{n}$

#### Mediana

Em um conjunto de valores ordenados, a mediana é o valor que ocupa a posição central. Portanto, a mediana divide a distribuição de valores na metade.

#### Moda
  Em um conjunto de valores, a Moda seria o valor que ocorre com maior frequência. Ou seja, o valor que mais se repete.
  
### Medidas de Variabilidade

#### Amplitude

Em um conjunto de observações, a Amplitude é a diferença entre o maior valor e o menor.

#### Variância

Para uma amostra de *n* observações, a Variância será:

 $s=\frac{\sum(x_{i}-\bar{x})^{2}}{n-1}$

#### Desvio Médio (*Standard Deviation*)

Para uma amostra de *n* obervações, o Desvio Padrão (SD) será a raiz quadrada positiva da Variância.

#### Quartis, Decis e Percentis

Os **Quartis** dividem um conjunto de obervações ordenados em 4 partes iguais; os **Decis**, em 10; e, os **Percentis**, em 100.

### Medidas de Forma

As medidas de forma permitem a verificação de como um conjunto de dados está se comportando em sua distribuição. Gráficos de distribuição de frequência e histogramas são ferramentas importantes para essa verificação.

#### Assimetria

Distribuições em forma de sino são simétricas, já que a média, mediana e moda desse conjunto de dados são iguais. Ou seja, a metade esquerda do histograma é aproximadamente igual à metade direita.

Distribuições assimétricas possuem uma concentração de seus dados, à direita ou à esquerda, no histograma. Ou seja, apresentam uma "cauda" em uma das extremidades. Se a distribuição desses dados estiverem concentrados à direita, são dados com **assimetria positiva**. Se concentrados à esquerda, **assimetria negativa**.

##### Coeficiente de Assimetria de Pearson

$A_s=\frac{3*(\bar{x} - Md)}{s}$

#### Curtose

Curtose indica o grau de achatamento de uma distribuição em relação à curva normal. 

##### Coeficiente de Curtose

$C=\frac{Q_3-Q_1}{2*(P_{90} - P_{10})}$

A curva normal possui C=0,263.

## Aplicação {-}

Para a aplicação, será utilizada a base de dados do Instituto Nacional de Meteorologia (INMET) de precipitação e temperatura na estação meterológica do Mirante de Santana em São Paulo, dos anos de 2014 a 2016. 

No ano de 2014, São Paulo passou pela pior estiagem deste o ano de 1953. Entre o fim de 2014 e o outono de 2016, a cidade registrou o maior El Niño deste 1950. O El Niño, quando ocorre, causa um aquecimento anômalo das águas superficiais e sub-superficiais do Oceano Pacífico Equatorial, porém, não se comporta de forma constante em relação ao volume de chuvas. Uma das dúvidas do período era se o El niño poderia afetar o regime de chuvas e, assim, aumentar a precipitação em São Paulo. 

Portanto, iremos aplicar a EDA para verificar essa relação entre o El niño e o volume de chuvas.

Para importar a base de dados em .csv, utiliza-se a função read.csv() e insere-se como argumento o *diretório*:

```{r}

met<- read.csv2("~/UFABC Beatriz/TG Beatriz Lima/Dados/data1.csv")

met1 <- na.omit(met) # Omitindo os valores faltantes do conjunto de dados
```

É importante lembrar que read.csv() é utilizado quando os valores são separados por vírgula e decimais por ponto; e, read.csv2(), quando são separados por ponto e vírgula e os decimais por vírgula.

Para visualizar a tabela importada, basta executar a função View():
```{r}

View(met1)

```

Para iniciar a exploração dos dados, inicialmente será utilizada a função str() que exibe de forma compacta a estrutura da tabela importada.

```{r}

str(met1)

```

Continuando, será utilizada a função ´summary()´ para apresentar o sumário de estatísticas descritivas (média, mediana, mínimo, máximo, 1º quartil, 3º quartil e valores faltantes (NA)).

```{r}

summary(met1)

```
Para continuar explorando os dados, realizou-se um gráfico de linhas a fim de analisar o comportamento da precipitação em relação às datas.

```{r}

year <- met1$Data
precip <- met1$PRECIPITACAOTOTAL

df <- data.frame(precipitacao=precip, ano=year)

library(ggplot2)

ggplot(df,
       aes(x = ano, y = precipitacao)) +
       geom_line(linetype="dashed", color="blue", aes(group=1)) +
       geom_point()

```

A distribuição de frequência da precipitação apresenta assimetria à direita (positiva):

```{r}

ggplot(data = df, 
      aes(x = precip)) +
      geom_histogram() 
  

```

Após realizar a sumarização numérica de todos os dados, agora será focado nos dados de interesse: precipitação e ano. Primeiramente, será encontrada a média de precipitação por ano analisado, com um intervalo de confiança de 95% (escore-x da curva normal igual a 1,96).

```{r}

#Média das precipitações por ano

library(dplyr) #instalando o pacote para utilizar o operador pipe %>%

tabela_stats <- met1 %>%
  group_by(ANO) %>%
  summarise(n_obs = n(), media = mean(PRECIPITACAOTOTAL), desvio_padrao = sd(PRECIPITACAOTOTAL)) %>% 
  mutate(erro = 1.96*desvio_padrao/sqrt(n_obs), 
         limite_superior = media + erro,
         limite_inferior = media - erro)

```

Com o objetivo de analisar a graficamente os valores de precipitação por ano, será utilizado um gráfico de barras com os erros padrões das amostras. Utiliza-se o erro padrão e não o desvio padrão porque, neste caso, estamos interessados na variabilidade das médias das amostras e não na variabilidade das observações dentro da amostra.

```{r}
#Visualizando a tabela criada com os dados estatísticos
View(tabela_stats)

#Plotando os valores com seus respectivos erros
ggplot (data = tabela_stats, aes(x=ANO, y = media, fill=ANO)) +
  geom_col() +
  geom_errorbar(aes(ymin=limite_inferior, ymax=limite_superior)) +
  ggtitle("Média das precipitações por ano") +
  xlab("") +
  ylab("") +
  theme_bw() +
  theme(legend.position = "none")

```

É possível analisar que houve um aumento do volume de chuvas no ano de 2015, se comparado com o ano de 2014. O valor médio de precipitação de 2016 é ligeiramente maior que a média de 2014. Porém, não é possível afirmar que os valores de precipitação se devem somente ao El niño, porque também existem diversos fatores, não analisados aqui, que podem influenciar no volume de chuvas.

<!--chapter:end:01-intro.Rmd-->

# Modelos lineares

Resumidamente, modelos lineares são utilizados para resumir relações observadas a partir de dados, no caso, ambientais, em uma linha reta. Em um modelo linear descreve-se o comportamento de uma variável dependente (ou variável resposta, y) como função de uma ou mais variáveis independentes (ou variáveis explicativas, x). 

## Covariância
A *Covariância*, denotada $\sigma_{xy}$, descreve a variação, em relação a média, entre duas variáveis. 

$cov(x,y) = \frac{\sum(x_i - \overline{x})}{n-1}$

Portanto, é possível saber se ambas variáveis desviam na mesma direção (covariância positiva) ou se desviam em direções opostas (covariância negativa). Caso a covariância entre duas variáveis seja zero, a conclusão é que as variáveis são independentes. 


## Correlação

A **Correlação** é útil para medir a relação linear entre duas variáveis x e y, denotada por $\rho_{xy}$.

$\rho_xy = \frac{cov(x,Y)}{\sqrt(V(x)V(Y))}$

Sendo assim, duas variáveis podem estar relacionadas das seguintes formas:

  1. Positivamente relacionadas. Ou seja, se x aumenta, y aumenta. E o mesmo ocorre para caso X diminua (Y diminui) -  Correlação positiva;
  2. Negativamente relacionadas. Ou seja, se x aumenta, y diminui. E o mesmo ocorre para caso x diminua (Y aumenta) - Correlação negativa;
  3. Não há relação entre as duas variáveis.

Uma forma gráfica de visualizar a correlação das variáveis que apresenta grande utilidade é a partir do **Diagrama de Dispersão**. O diagrama traz informações importantes porque mostra se a relação entre as variáveis é linear ou não, se existem outliers no conjunto de dados e traz uma ideia de quão forte é o relacionamento entre as variáveis.

É importante afirmar que o valor não implica causalidade, mas quantifica a relação entre as variáveis selecionadas.

### Coeficiente de Pearson (r)

O *Coeficiente de Pearson*, também chamado de coeficiente de correlação da amostra $r_{xy}$, mede a força da relação linear entre duas variáveis aleatórias x e y. Se duas variáveis relacionarem-se perfeitamente com inclinação positiva, $r_{xy}=1$; com inclinação negativa, $r_{xy}=-1$; se $r_{xy}=0$, não há relação entre as variáveis. 

De acordo com Filho e Júnior (2009), existem métricas diferentes de acordo com cada autor:

- Cohen (1998) considera valores entre 0,10 e 0,29 pequenos; entre 0,30 e 0,49, médios; e entre 0,50 e 1, grandes.

- Dancey e Reidy (2005) considera valores entre 0,10 e 0,30 pequenos; entre 0,40 e 0,60 moderados; e de 0,70 até 1, grandes.

O consenso é que quanto mais próximo de 1, maior a força da relação linear entre as variáveis, independente do sinal.

## Regressão linear simples

A partir da regressão, é possível obter a relação matemática que descreva a relação entre duas ou mais variáveis. A análise de regressão é uma coleção de ferramentas estatísticas que permite a modelagem e inferência de uma variável dependente (y) com uma ou mais variáveis independentes (x). No caso da **regressão linear simples**, somente existe uma variável independente; para a regressão linear múltipla, mais de uma. Dessa forma, o formato básico do modelo de regressão linear é:

$Y = \beta_0 + \beta_1X_1 + e_1$, sendo $\beta_0$ o coeficiente do intercepto, $\beta_1$ o coeficiente de inclinação, $e_1$ o erro no ajuste do modelo para a observação y.

De acordo com Montgomery & Runger (2021), o modelo de regressão é, na verdade, uma linha de valores médios. Ou seja, "*a altura da linha de regressão em qualquer valor de x é apenas o valor esperado de Y para aquele x. O coeficiente angular,* $\beta_1$*, pode ser interpretado como a mudança na média de Y para uma mudança unitária em x. Além disso, a variabilidade de Y, em um valor particular de x, é determinada pela variância do erro* $\sigma^2$". Portanto, há uma distribuição de valores de Y em cada x de forma que a variância da distribuição é constante em cada x. 

O erro $e_1$ é estimado pela variabilidade de Y que o modelo criado não consegue explicar, ou seja, o resíduo pode ser quantificado por $\widehat{Y}-Y$. Os resíduos conseguem indicar se as suposições do modelo foram violadas e, por isso, agora conheceremos as condições necessárias para a aplicação da Regressão Linear.

### Condições

São assumidas algumas hipóteses sobre os dados de entrada na Regressão Linear:

1. Linearidade: a relação entre X e Y deve ser linear;

2. Homocedasticidade: a variância da variável dependente (Y) deve ser constante para todos os valores das variáveis independentes (X);

3. Normalidade: para um valor fixo de X, Y é uma variável aleatória com distribuição normal. Os erros também devem ser normalmente distribuídos;
Dentre diversos testes possíveis, para verificar a não-normalidade dos erros é possível realizar o Teste de Shapiro-Wilk ou um gráfico de Probabilidade Normal com objetivo de verificar visualmente se os dados do modelo apresentam distribuição normal.

3. Independência dos resíduos: como Yi e Yj são valores estatisticamente independentes (falta de correlação), os resíduos também deverão ser independentes;

4. Ausência de outliers influentes: não devem existir outliers que influenciem consideravelmente o modelo;


### Coeficiente de Determinação (R²)

O *Coeficiente de Determinação* é utilizado frequentemente pra avaliar a adequação de um modelo de regressão. É definido pelo quadrado do coeficiente de correlação entre X e Y. Quanto mais próximo de 1, maior a quantidade de variabilidade nos dados explicada pelo modelo de regressão. 

De acordo com Montgomery (2021), o R² pode trazer interpretações errôneas, já que sempre é possível fazer com que R² seja unitário realizando a adição de mais termos ao modelo. Ou seja, R² aumenta se for adicionado uma nova variável ao modelo, mas isso não indica que o modelo esteja mais adequado.



## Aplicação {-}

Para a aplicação serão utilizados dados de emissões dos gases de efeito estufa (GEE) por mudanças de cobertura da terra da Amazônia Legal disponibilizados pelo INPE (Instituto Nacional de Pesquisas Espaciais). Será representada a relação entre a área desmatada por ano e a emissão de 1ª ordem de $CO_2$ na Amazônia Legal.A estimativa de 1ª ordem supõe que, de modo simplificado, 100% das emissões ocorreram no momento da mudança de uso/cobertura.

Um **primeiro passo** será carregar os dados:

```{r}
options("install.lock"=FALSE)

## Carregar pacotes que serão usados


## Checando repositório
setwd("C:/Users/Luiz Arthur/Dropbox/PC/Documents/UFABC Beatriz/TG Beatriz Lima/Dados")

## Carregando base de dados
dados0 <- read.csv2("CO2Amazonia.csv")
dados <- na.omit(dados0)

View(dados)
data_size= dim(dados)

```
Agora que os dados foram carregados, o **segundo passo** será entender os dados. Para saber algumas propriedade dos dados carregados, será utilizado o comando ``head()`, que apresentará uma amostra dos dados, e ``summary()`, que apresenta as estatísticas básicas dos dados (média, mediana, 1º quartil, etc).

```{r}

head(dados)

summary(dados)

```
Como iremos analisar a relação entre a área desmatada por ano (D_Area) e a emissão de 1ª ordem de $CO_2$, é importante verificar qual é o comportamento entre esses dados.

```{r}

## Verificando a relação entre a variável dependente e a variável independente
plot(dados$D_Area, dados$VR_CO2_1stOrder)

## Correlação entre as variáveis da base de dados
cor.test(dados$VR_CO2_1stOrder,dados$D_Area)


```

A partir desse gráfico, é possível verificar que a relação entre as variáveis é linear e, dessa forma, conforme a área desmatada aumenta, a emissão de $CO_2$ aumenta linearmente. O valor da correlação indica que a relação entre as duas variáveis é forte e positiva, já que 0,9987358 é próximo de 1 e maior que zero.

```{r}

## Construção do modelo
mod <- lm(VR_CO2_1stOrder ~ D_Area, dados, na.action = na.exclude)
summary(mod)


```
O r² (coeficiente de determinação) do modelo é 0,9975 e, portanto, pode-se interpretar que a variável área explica 99,75% da variação na emissão de $CO_2$ O valor indica que o modelo possui bom ajuste. 

O p-value do modelo apresenta valor $2.2e^{-16}$ e, assim, apresenta valor menor que o nível de significância (0,05), mostrando que existe baixa probabilidade dos resultados apresentados pelo modelo não possuírem erro amostral. Ou seja, existe alta probabilidade do modelo não ser um bom ajuste. Isso continuará sendo testado a diante.

De acordo com Montgomery e Runger (2021), "*A análise dos resíduos é frequentemente útil na verificação da suposição de que os erros sejam distribuídos de forma aproximadamente normal, com variância constante, assim como na determinação da utilidade dos termos adicionais no modelo*". Dessa forma, abaixo será realizada a análise residual.

```{r}
# Análise dos resíduos

plot(mod,which = 4)

par(mfrow=c(2,2)) 
plot(mod)


## Teste de normalidade
shapiro.test(mod$residuals)

```

Para a análise residual, é necessário investigar se os resíduos refletem as propriedades impostas pelo erro do modelo. Os resíduos não podem apresentar uma tendência e, por isso, eles serão analisado abaixo.
 
 - Residuals vs Fitted
 
O gráfico mostra a relação entre os resíduos e os valores ajustados. Como a distribuição dos resíduos próxima à linha pontilhada demonsta um bom ajuste do modelo, é possível verificar se resíduos tem padrões não-lineares. No caso do nosso modelo, os resíduos não se apresentam próximos à linha pontilhada.

As observações 48, 50 e 51 apresentam grandes valores de resíduos e, por isso, é interessante realizar toda a análise após a remoção dessas observações.
 
 - Normal Q-Q
 
O gráfico mostra se os resíduos são normalmente distribuídos. Mais uma vez, o ideal é que a distribuição dos resíduos acompanhe a linha pontilhada. Pelo gráfico é possível perceber que a distribuição dos resíduos está diferente da distribuição normal. As observações 48, 50 e 51 apresentam-se extremas novamente.
  
 - Scale-Location
 
O gráfico mostra se os resíduos são igualmente distribuídos em relação ao intervalo de preditores (Fitted values).Também é possível checar a homocedasticidade. O ideal, no caso, seria que os resíduos estivessem uniformemente distribuídos ao redor da linha vermelha. Para o caso do nosso modelo, demonstra que há heterocedasticidade, ou seja, os resíduos não estão uniformemente distribuídos em relação ao intervalo de preditores.
  
 - Residuals vs Leverage
 
O gráfico ajuda na visualização de possíveis casos influentes, ou seja, outliers que influenciam na análise de regressão linear. O que quer dizer que, sem o outlier, o resultado da regressão seria diferente. No caso do modelo criado, há possibilidade de existirem outliers influentes.

 - Cook's distance
 
A Distância de Cook informa o quanto um caso é capaz de influenciar o modelo de regressão. Portanto, o gráfico estima a influência de cada observação no modelo e, novamente, as observações 48, 50 e 51 são extremas. 

```` {r}
CD <- cooks.distance(mod)
influentes <- CD[(CD > (3* mean(CD, na.rm=TRUE)))]

print(influentes)
````
É possível analisar que existem 6 observações que possuem uma distância de Cook três vezes maior que a média. Além disso, é confirmado que as observações 48, 50 e 51 são extremas.

Pelo Teste de Shapiro, é possível verificar que o p-value é menor que 0,05, portanto, o valor não é adequado. O gráfico Normal Q-Q serve como uma contra-prova, também mostrando que os dados não são normalmente distribuídos e, dessa forma, não são adequados.

Os resultados dos resíduos indicam no mínimo uma necessidade de aumento do número de dados ou uma amostra mais representativa. Além disso, outra alternativa seria a existência de outliers. Para realizar o teste de outliers, é interessante utilizar os gráficos Boxplot e Histograma. 

```{r}

#Encontrando potenciais outliers a partir de gráficos

hist(dados$D_Area)

hist(dados$VR_CO2_1stOrder)

```

A partir da análise dos resíduos, foi possível inferir que as observações 48, 50 e 51 são outliers influentes. Possivelmente, em relação a área, o outlier está abaixo de 500000 m²; enquanto para a emissão de CO2, acima de 1400 ppm. Para continuar procurando esses possíveis outliers, o pacote `outliers` pode ser utilizado, já que a função `outlier()` consegue encontrar o valor mais distante da média das variáveis.

```{r}
#Encontrando os valores com maior diferença da média com o pacote `outliers`

library(outliers)


outArea <- outlier(dados$D_Area)

outCO2 <- outlier(dados$VR_CO2_1stOrder)

print(outArea)

print(outCO2)

```

Com esse resultado, analisa-se que existe grande possibilidade de existirem outliers no conjunto de dados, já que as observações 48, 50 e 51 apresentam-se extremas e influentes no modelo de regressão. Além disso, a hipótese de que o resultado dos resíduos indica que seja necessário um maior conjunto de dados também é uma possibilidade.

Após todos os testes, por fim, o resultado do modelo de regressão linear simples pode ser visualizado abaixo.

```{r}

#Diagrama de dispersão com o ajuste

plot(x = dados$D_Area, y = dados$VR_CO2_1stOrder, xlab = "Área desmatada no ano", ylab = "Emissão de CO2 de 1a ordem")

abline(mod, col = "blue")
```

Pelas estatísticas, foi possível analisar que as variáveis relacionam-se de forma positiva e linear, além do modelo apresentar um R² satisfatório. Porém, após a análise dos resíduos, foi possível concluir que o modelo, apesar de ter certas estatísticas boas, não representa de forma adequada a relação entre o desmatamento anual e a emissão de $CO_2$ na Amazônia Legal. Portanto, torna-se importante refazer o modelo, de forma que os outliers influentes (observações 48, 50, 51, 52, 60 e 61) sejam retirados, para verificar se este novo modelo estaria mais adequado para representar a relação entre as variáveis. 

É isso que faremos:

```{r}
nomes_influentes <- names(influentes)
out_influentes <- dados[nomes_influentes,]
dados_sem_outliers <- dados[!dados %in% out_influentes]

length(dados)
length(dados_sem_outliers)

View(dados_sem_outliers2)

mod2<- lm(VR_CO2_1stOrder ~ D_Area, data=dados_sem_outliers)

summary(mod2)

plot(mod2)
```

Referências Bibliográficas {-}

Montgomery, Douglas C.; Runger, George C.. **Estatística aplicada e probabilidade para engenheiros**. tradução e revisão técnica Veronica Calado, Antonio Henrique Monteiro da Fonseca Thomé da Silva · - 7. ed. - Rio de Janeiro : LTC, 2021.

SIDHU, Rishi. **Layman's Introduction to Linear Regression**. Disponível em: <https://towardsdatascience.com/laymans-introduction-to-linear-regression-8b334a3dab09> 

R DOCUMENTATION. *shapiro.test(x)*. Disponível em: <https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/shapiro.test>.

FILHO, Dalson Britto Figueiredo; JÚNIOR, José Alexandre da Silva. *Desvendando os Mistérios do Coeficiente de Correlação de Pearson (r)*. Revista Política Hoje vol. 18, n. 1,  2009. Disponível em: <https://periodicos.ufpe.br/revistas/politicahoje/article/viewFile/3852/3156>.

THIEME, Christian. Identifying Outliers in Linear Regression — Cook’s Distance. Towards Data Science, 2021. Disponível em: <https://towardsdatascience.com/identifying-outliers-in-linear-regression-cooks-distance-9e212e9136a#:~:text=One%20method%20that%20is%20often,the%20ith%20observation%20is%20removed>.

<!--chapter:end:02-cross-refs.Rmd-->

# Parts

You can add parts to organize one or more book chapters together. Parts can be inserted at the top of an .Rmd file, before the first-level chapter heading in that same file. 

Add a numbered part: `# (PART) Act one {-}` (followed by `# A chapter`)

Add an unnumbered part: `# (PART\*) Act one {-}` (followed by `# A chapter`)

Add an appendix as a special kind of un-numbered part: `# (APPENDIX) Other stuff {-}` (followed by `# A chapter`). Chapters in an appendix are prepended with letters instead of numbers.




<!--chapter:end:03-parts.Rmd-->

# Footnotes and citations 

## Footnotes

Footnotes are put inside the square brackets after a caret `^[]`. Like this one ^[This is a footnote.]. 

## Citations

Reference items in your bibliography file(s) using `@key`.

For example, we are using the **bookdown** package [@R-bookdown] (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and **knitr** [@xie2015] (this citation was added manually in an external file book.bib). 
Note that the `.bib` files need to be listed in the index.Rmd with the YAML `bibliography` key.


The RStudio Visual Markdown Editor can also make it easier to insert citations: <https://rstudio.github.io/visual-markdown-editing/#/citations>

<!--chapter:end:04-citations.Rmd-->

# Blocks

## Equations

Here is an equation.

\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation} 

You may refer to using `\@ref(eq:binom)`, like see Equation \@ref(eq:binom).


## Theorems and proofs

Labeled theorems can be referenced in text using `\@ref(thm:tri)`, for example, check out this smart theorem \@ref(thm:tri).

::: {.theorem #tri}
For a right triangle, if $c$ denotes the *length* of the hypotenuse
and $a$ and $b$ denote the lengths of the **other** two sides, we have
$$a^2 + b^2 = c^2$$
:::

Read more here <https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html>.

## Callout blocks


The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html

<!--chapter:end:05-blocks.Rmd-->

# Sharing your book

## Publishing

HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html

## 404 pages

By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you'd like to customize your 404 page instead of using the default, you may add either a `_404.Rmd` or `_404.md` file to your project root and use code and/or Markdown syntax.

## Metadata for sharing

Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the `index.Rmd` YAML. To setup, set the `url` for your book and the path to your `cover-image` file. Your book's `title` and `description` are also used.



This `gitbook` uses the same social sharing data across all chapters in your book- all links shared will look the same.

Specify your book's source repository on GitHub using the `edit` key under the configuration options in the `_output.yml` file, which allows users to suggest an edit by linking to a chapter's source file. 

Read more about the features of this output format here:

https://pkgs.rstudio.com/bookdown/reference/gitbook.html

Or use:

```{r eval=FALSE}
?bookdown::gitbook
```



<!--chapter:end:06-share.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`
The R Foundation. **R: The R Project**. Disponível em: <https://www.r-project.org/>

Wickham, Hadley; Grolemund, Garrett. **R for Data Science**. Disponível em: <https://r4ds.had.co.nz/exploratory-data-analysis.html>

Montgomery, Douglas C.; Runger, George C.. **Estatística aplicada e probabilidade para engenheiros**. tradução e revisão técnica Veronica Calado, Antonio Henrique Monteiro da Fonseca Thomé da Silva · - 7. ed. - Rio de Janeiro : LTC, 2021.

SIDHU, Rishi. **Layman's Introduction to Linear Regression**. Disponível em: <https://towardsdatascience.com/laymans-introduction-to-linear-regression-8b334a3dab09> 

DAVIS, Jerry. Introduction to Environmental Data Science. SFSU Institute for Geographic Information Science, 2022. Disponível em: https://bookdown.org/igisc/EnvDataSci/#environmental-data-science. Acesso em: 27 abr. 2022.

FILATRO, Andrea C. Data science da educação. [Digite o Local da Editora]: Editora Saraiva, 2020. 9786587958446. Disponível em: https://integrada.minhabiblioteca.com.br/#/books/9786587958446/. Acesso em: 27 abr. 2022.

Gibert, Karina; Horsburgh, Jeffery S.; Athanasiadis, Ioannis N.; Holmes, Geoff. Environmental Data Science. Environmental Modelling & Software, 2018. Disponível em: <https://reader.elsevier.com/reader/sd/pii/S1364815218301269?token=6B7E02806686B9D1213F7D7D568DD8607AC7EDC45701E1AB2FF569284245D3B9B73444E8F4532FFF767F851CCECF7289&originRegion=us-east-1&originCreation=20220429000236>. Acesso em: 28 abr. 2022.

R DOCUMENTATION. *shapiro.test(x)*. Disponível em: <https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/shapiro.test>

<!--chapter:end:07-references.Rmd-->

