[["análise-de-séries-temporais.html", "Capítulo 6 Análise de séries temporais", " Capítulo 6 Análise de séries temporais Qualquer conjunto de observações ordenadas no tempo são chamadas de Séries Temporais. No caso da Engenharia Ambiental e Urbana, podem ser citados alguns exemplos: valores diários de poluição, valores mensais de temperatura, temperaturas máximas e mínimas diárias ou precipitação atmosférica anual em uma cidade, entre outros (Morettin 2018). É suposto que, em uma análise de série temporal, “há um sistema causal mais ou menos constante, relacionado com o tempo, que exerceu influência sobre os dados no passado e pode continuar a fazê-lo no futuro. Este sistema causal costuma atuar criando padrões não aleatórios que podem ser detectados em um gráfico da série temporal, ou mediante algum outro processo estatístico” (Reis 2022). É importante citar que identificar esses padrões não aleatórios na série temporal de uma variável de interesse é o objetivo da análise. A partir dessa identificação, é possível realizar previsões, orientando tomadas de decisões na área de interesse (Reis 2022). A partir de procedimentos estatísticos, é possível realizar a análise da série de dados de forma que seja possível realizar previsões, analisar causalidades e estudar periodicidades relevantes. Conforme COSTA (2022) e SOARES (2020), em resumo, nas séries temporais caracterizam-se por: Possuirem variáveis aleatórias; Observações altamente correlacionadas, principalmente porque medidas adjacentes (próximas no tempo) tendem a correlatas; Existência de fatores determinísticos, como tendência, sazonalidade e ciclos; Dificuldade em lidar com outliers; Mudanças fundamentais/abruptas. De acordo com Reis (2022) e Barros, D., and D. (2017), séries temporais são compostas pelos seguintes padrões: Tendência (T): série temporal que segue determinada direção, não necessariamente linear, podendo também ser crescente ou decrescente. Exemplos podem ser: crescimento demográfico, ou mudança gradual de hábitos de consumo, ou qualquer outro aspecto que afete a variável de interesse no longo prazo; Variações cíclicas ou ciclos (C), flutuações nos valores da variável com duração superior a um ano, e que se repetem com certa periodicidade, que podem ser resultado de variações da economia como períodos de crescimento ou recessão, ou fenômenos climáticos como o El Niño (que se repete com periodicidade superior a um ano); Variações sazonais ou sazonalidade (S), flutuações nos valores da variável com duração inferior a um ano, e que se repetem todos os anos, geralmente em função das estações do ano (ou em função de feriados ou festas populares, ou por exigências legais, como o período para entrega da declaração de Imposto de Renda); se os dados forem registrados anualmente NÃO haverá influência da sazonalidade na série; Variações irregulares (I), que são as flutuações inexplicáveis, resultado de fatos inesperados como catástrofes naturais, decisões intempestivas de governos, etc. Estes são estudados para controlar o efeito de cada componente a fim de analisar o comportamento de cada “série filtrada” e, ainda, analisar as particularidades de cada componente (COSTA, 2022). (#fig:figura 3.1)Figura 3.1: Séries exibindo diferentes tipos de padrões (Fonte: BARROS et. al, 2022) De acordo com COSTA (2022), existem duas abordagens na análise de séries temporais: Clássica: descreve o comportamento da série por meio de componentes não observáveis, ou seja, a tendência, sazonalidade e ciclo. Os desvios em torno dos valores determinísticos1 ocorrem por erros aleatórios e não pela natureza estocástica da série. Moderna: supões que a série temporal seja formada por um processo estocástico2. Sendo cada observação composta por um conjunto de variáveis aleatórias, estas são representadas por funções de densidade individuais. Para capturar sua característica estocástica, são utilizadas modelos estatísticos. References "],["estacionaridade.html", "6.1 Estacionaridade", " 6.1 Estacionaridade Para realizar a abordagem de séries temporais assume-se que esta seja formada por um processo estocástico, como citado anteriormente. De forma resumida, uma série estacionária possui a mesma distribuição de probabilidade para cada observação. A estacionaridade divide-se em: forte, em que a função densidade de probabilidade não varia no tempo e as distribuição são iguais para todo \\(t\\); fraca, em que a média, variância e covariância permanecem constantes ao longo do tempo. A estacionaridade fraca é menos restritiva que a forte e, por isso, será a abordagem escolhida. Uma série fracamente estacionária tem as seguintes características: A média é igual a uma constante A variância é constante e finita; A covariância é dependente somente da diferença no tempo (h) entre duas medidas, sendo a covariância: \\[\\tag{4.3}Cov(X_t,X_{t+h})=\\sum(X_t-\\overline{X_t})(X_{t+h}-\\overline{X_{t+h}})\\], em que \\(X_t\\) é a própria série temporal e \\(h\\) é o intervalo de tempo entre as duas medidas. Dessa forma, quando a covariância é positiva, a tendência é positiva entre \\(X_t\\) e \\(X_{t+h}\\). Quando a covariância é negativa, a tendência também é negativa. Se a covariância for nula, não existe tendência. É importante citar que se uma série não for estacionária, modelos padrões podem não representar de forma adequada o processo gerador da série temporal e, por isso, não seria possível utilizar o modelo para realizar previsões ou outros objetivos da análise. Conforme o Teorema de Wald, a estacionariedade dá estabilidade e coerência ao modelo. "],["operador-de-diferenças.html", "6.2 Operador de diferenças", " 6.2 Operador de diferenças Dada uma série temporal {\\(y_t=y_{t=1}\\)}, o operador de diferenças transforma a série original em uma nova série formada por diferenças sucessivas: \\(\\nabla^dy_t\\) é uma série diferenciada “d” vezes, sendo que \\(\\nabla y_t=y_t - y_{t-1}\\) é a primeira diferença da série. Dessa forma, a segunda diferença da série será: \\(\\nabla^2y_t = \\nabla(\\nabla y_t) = \\nabla (y_t - y_{t-1}) = \\nabla y_t - \\nabla y_{t-1} = y_t - 2y_{t-1} + y_{t-2}\\) Para a n-ésima diferença da série: \\(\\nabla^n y_t = \\nabla(\\nabla y_t) = \\sum_{r=0}^n (-1)^r \\binom{n}{r} y_{t-r}\\), em que \\(\\binom{n}{r}=\\frac{n!}{r!(n-r)!}\\). É mais comum que uma ou duas diferenças já sejam suficientes para tornar a série estacionária. "],["ruído-branco.html", "6.3 Ruído branco", " 6.3 Ruído branco No ruído branco, toda a trajetória da série temporal é atribuida a fatores puramente aleatórios. Ou seja, variáveis aleatórias não correlacionadas em um processo estocástico \\({\\epsilon_t, t \\in T}\\) possuem média zero e variância constante, portanto, \\(\\epsilon_t\\sim RB(0,\\sigma^2)\\). É representado como: \\[\\tag{4.5}y_t=\\epsilon_t\\] "],["passeio-aleatório.html", "6.4 Passeio aleatório", " 6.4 Passeio aleatório O passeio aleatório é representado por \\(\\tag{4.6}y_t=y_{t-1}+\\epsilon_t\\) em que \\(\\epsilon_t\\sim RB(0,\\sigma^2)\\). Portanto, o valor da série em \\(t\\) é igual ao valor da série em um instante anterior somado a um erro de média zero e variância constante. É possível escrever o passeio aleatório em função de \\(y_0\\): \\[y_t=y_0+\\sum_{i=1}^t{\\epsilon_i}\\] ### Testes de raiz unitária A raiz unitária indica que a série é uma realização de um processo estocástico não-estacionário. Portanto, uma série com raiz unitária precisa ser diferenciada para se tornar estacionária: \\[ y_t= y_{t-1} + \\epsilon_t\\] \\[y_t-(y_{t-1}) = y_{t-1} - (y_{t-1}) + \\epsilon_t\\] \\[\\nabla y_t= e_{1t}\\] O número de diferenças necessárias para tornar uma série estacionária é chamado de ordem de integração da série. No exemplo acima, a primeira diferença de um passeio aleatório é estacionária e, portanto, passa a ser um ruído branco. Um processo é dito ser integrado de ordem d se uma série temporal {\\(y_t_{t=1}^T\\)} é não estacionária. Porém, após d diferenças, o resultado é uma série temporal estacionária e, portanto, pode ser modelada por um modelo ARMA(p,q). Para realizar o teste da raiz unitária, os mais utilizados é o teste Aumentado de Dickey-Fuller (ADF). "],["modelos-autoregressivos.html", "6.5 Modelos autoregressivos", " 6.5 Modelos autoregressivos Modelos autoregressivos AR(p) é caracterizado por realizar a modelagem somente a partir das defasagens de uma série temporal, ou seja: \\[y_t=\\phi_0+\\phi_1y_{t-1}+\\phi_2y_{t-2}+...+\\phi_py_{t-p}+ \\epsilon_t\\] ## Médias Móveis O modelo de médias móveis MA(q) modela a partir das defasagens do termo de erro \\(\\epsilon_t\\), sendo assim uma combinação linear de ruídos brancos (\\(\\epsilon+t\\) tem média zero, variância constante e não-autocorrelacionado). \\[y_t=\\mu+\\theta_1\\epsilon_{t-1}+\\theta_2\\epsilon_{t-2}+...+\\theta_p\\epsilon_{t-q}+ \\epsilon_t\\] "],["modelos-arma.html", "6.6 Modelos ARMA", " 6.6 Modelos ARMA Um modelo ARMA (p,q) é apresentado como: \\[\\overline Z_t = \\phi_1\\overline Z_{t-1}+...+\\phi_p\\overline Z_{t-p}+\\alpha_t-\\theta_1\\alpha_{t-1}-...-\\theta_q\\alpha_{t-q}\\] Sendo que \\(\\phi(B)\\) e \\(\\theta(B)\\) são, respectivamente, os operadores autoregressivos e de médias móveis. Dessa forma: \\[\\phi(B) \\overline Z_t=\\theta(B) \\alpha_t\\] A determinação das ordens p e q do modelo pode ser realizada a partir da análise gráfica das função de autorrelação FAC (Função de Autocorrelação) e FACP (Função de Autocorrelação Parcial). A FAC determina a ordem de médias móveis e FACP a ordem autoregressiva. "],["modelos-arima.html", "6.7 Modelos ARIMA", " 6.7 Modelos ARIMA Um modelo ARIMA (p,d,q) (Modelos autoregressivos integrados de médias móveis), a partir da abordagem de Box &amp; Jenkins, permite que previsões sejam realizadas tomando por base valores presentes e passados da série temporal. O modelo é formado por três componentes: o Auto Regressivo (AR), o filtro de integração (I) e o componente de médias móveis. Um modelo AR(p) é baseado na ideia de que um valor atual \\(y_t\\) pode ser explicado como uma função de valores passados. Este pode ser descrito como: \\[y_t=c+\\phi_1y_{t-1}+...+\\phi_py_{t-p}+\\epsilon_t\\] onde \\(\\phi_p\\) são parâmetros auto-regressivos; \\(c\\) é uma constante e \\(\\epsilon_t \\sim RB(0,\\sigma^2)\\) Já um modelo MA(q): \\[y_t=\\mu+\\epsilon_t-\\theta_1\\epsilon_{t-1}-...-\\theta_q\\epsilon_{t-q}\\], onde \\(\\theta_n\\) são parâmetros de médias móveis; \\(c\\) é uma constante e \\(\\epsilon_t\\sim(0,\\sigma^2)\\). "],["modelos-sarima.html", "6.8 Modelos SARIMA", " 6.8 Modelos SARIMA Quando considerada a sazonalidade, o modelo é denominado SARIMA \\((p,d,q)(P,D,Q)_s\\), representado pela equação 4.15. \\[\\tag{4.15}\\phi(L)\\Phi(L)\\Delta^d\\Delta^Dy_t=\\theta(L)\\Theta(L)\\epsilon_t\\], em que: p: ordem do polinômio autoregressivo não sazonal \\(\\phi(L)\\) P: orgem do polinômio autoregressivo sazonal \\(\\Phi(L)\\) q: ordem do polinômio de médias móveis não sazonal \\(\\theta(L)\\) Q: ordem do polinômio de médias móveis sazonal \\(\\Theta(L)\\) d: ordem de diferença não sazonal D: ordem de diferença sazonal \\(\\phi(L)=(1-\\phi_1L-\\phi_2L^2-...-\\phi_pL^p\\) \\(\\Phi(L)=(1-\\Phi_1L^s-\\Phi_2L^{2s}-...-\\Phi_pL^{Ps})\\) \\(\\theta(L)=(1-\\theta_1L-\\theta_2L^2-...-\\theta_pL^q)\\) \\(\\Theta(L)=(1-\\Theta_1L^2-\\Theta_2L^{2s}-\\Theta_pL^{Qs})\\) \\(\\Delta-=1-L\\), sendo o operador de desafasagem (L): \\(L^ny_t=y_{t-n}\\) Conforme Morettin (2018), Na abordagem de Box &amp; Jenkins, existem 4 etapas: 1. Identificação do processo gerador de dados Para descobrir qual modelo descreve melhor o comportamento dos dados, inicialmente, realiza-se uma análise gráfica, verificando estacionaridade, tendência, sazonalidade, etc. A partir disso, utiliza-se a FAC e FACP (função de autocorrelação parcial) amostrais, além de verificar a ordem de integração da série, ou seja, número de diferenças necessárias para tornar uma série estacionária. É importante que a estacionariedade seja testada a partir dos testes de raiz unitária. Estimação de parâmetros Os parâmetros do modelo são identificados e testados estatisticamente e, para isso, utilizam-se critérios de informação. Verificação do modelo (diagnóstico) resíduos devem possuir média zero, variância constante e serem estacionários, comportando-se como um ruído branco. As funções de autocorrelação devem ser não significativas, ou seja, devem ficar dentro do intervalo de confiança. Pelo teste de Ljung-Box,avalia-se se o modelo sugerido é adequado. Verifica-e se os critérios AIC (Akaike) e BIC (Bayesiano) possuem valores menores. Caso haja qualquer problema na etapa, volta-se à etapa de Identificação. Previsão References "],["aplicação-1-1.html", "Aplicação 1", " Aplicação 1 Para a aplicação, foi utilizada uma base de dados de emissões de \\(CO_2\\) de diversos países ao longo dos anos de 1750 a 2020 da plataforma Kaggle. Além dos valores de emissão de \\(CO_2\\), também apresenta valores de emissão de metano e óxido nitroso. # Carregando base de dados # Fonte: https://www.kaggle.com/datasets/kvnxls/co2-emissions-dataset-1750-2020 emissions &lt;- read.csv(&quot;Data/co2data.csv&quot;, sep = &quot;,&quot;, dec = &quot;.&quot;) View(emissions) str(emissions) ## &#39;data.frame&#39;: 25989 obs. of 60 variables: ## $ iso_code : chr &quot;AFG&quot; &quot;AFG&quot; &quot;AFG&quot; &quot;AFG&quot; ... ## $ country : chr &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Afghanistan&quot; &quot;Afghanistan&quot; ... ## $ year : int 1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 ... ## $ co2 : num 0.015 0.084 0.092 0.092 0.106 0.106 0.154 0.183 0.293 0.33 ... ## $ co2_per_capita : num 0.002 0.011 0.012 0.012 0.013 0.013 0.019 0.022 0.034 0.038 ... ## $ trade_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ cement_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ cement_co2_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ coal_co2 : num 0.015 0.021 0.026 0.032 0.038 0.043 0.062 0.062 0.077 0.092 ... ## $ coal_co2_per_capita : num 0.002 0.003 0.003 0.004 0.005 0.005 0.008 0.007 0.009 0.011 ... ## $ flaring_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ flaring_co2_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ gas_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ gas_co2_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ oil_co2 : num NA 0.063 0.066 0.06 0.068 0.064 0.092 0.121 0.216 0.238 ... ## $ oil_co2_per_capita : num NA 0.008 0.008 0.008 0.008 0.008 0.011 0.014 0.025 0.027 ... ## $ other_industry_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ other_co2_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ co2_growth_prct : num NA 475 8.7 0 16 ... ## $ co2_growth_abs : num NA 0.07 0.007 0 0.015 0 0.048 0.029 0.11 0.037 ... ## $ co2_per_gdp : num NA 0.009 0.01 0.009 0.01 0.01 0.014 0.016 0.025 0.027 ... ## $ co2_per_unit_energy : num NA NA NA NA NA NA NA NA NA NA ... ## $ consumption_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ consumption_co2_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ consumption_co2_per_gdp : num NA NA NA NA NA NA NA NA NA NA ... ## $ cumulative_co2 : num 0.015 0.099 0.191 0.282 0.388 ... ## $ cumulative_cement_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ cumulative_coal_co2 : num 0.015 0.036 0.061 0.093 0.131 0.174 0.236 0.298 0.375 0.467 ... ## $ cumulative_flaring_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ cumulative_gas_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ cumulative_oil_co2 : num NA 0.063 0.129 0.189 0.257 0.321 0.413 0.534 0.75 0.988 ... ## $ cumulative_other_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ trade_co2_share : num NA NA NA NA NA NA NA NA NA NA ... ## $ share_global_co2 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ share_global_cement_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ share_global_coal_co2 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ share_global_flaring_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ share_global_gas_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ share_global_oil_co2 : num NA 0 0 0 0 0 0 0 0.01 0.01 ... ## $ share_global_other_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ share_global_cumulative_co2 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ share_global_cumulative_cement_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ share_global_cumulative_coal_co2 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ share_global_cumulative_flaring_co2: num NA NA NA NA NA NA NA NA NA NA ... ## $ share_global_cumulative_gas_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ share_global_cumulative_oil_co2 : num NA 0 0 0 0 0 0 0 0 0 ... ## $ share_global_cumulative_other_co2 : num NA NA NA NA NA NA NA NA NA NA ... ## $ total_ghg : num NA NA NA NA NA NA NA NA NA NA ... ## $ ghg_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ total_ghg_excluding_lucf : num NA NA NA NA NA NA NA NA NA NA ... ## $ ghg_excluding_lucf_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ methane : num NA NA NA NA NA NA NA NA NA NA ... ## $ methane_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ nitrous_oxide : num NA NA NA NA NA NA NA NA NA NA ... ## $ nitrous_oxide_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ population : num 7624058 7752117 7840151 7935996 8039684 ... ## $ gdp : num NA 9.42e+09 9.69e+09 1.00e+10 1.06e+10 ... ## $ primary_energy_consumption : num NA NA NA NA NA NA NA NA NA NA ... ## $ energy_per_capita : num NA NA NA NA NA NA NA NA NA NA ... ## $ energy_per_gdp : num NA NA NA NA NA NA NA NA NA NA ... Como o nosso interesse são as emissões do Brasil, foi realizada uma nova base de dados com somente as emissões brasileiras de \\(CO_2\\) ao longo dos anos. # Filtrando somente as emissões do Brasil emissions_br &lt;- emissions %&gt;% filter(country == &#39;Brazil&#39;) View(emissions_br) Porém, gostaríamos de analisar somente as emissões de \\(CO_2\\) do Brasil. Dessa forma, iremos separar em uma base de dados somente os dados que interessam para a análise: País e Emissão de \\(CO_2\\). # Separando somente as informações importantes para a análise # Para a análise, precisamos somente da coluna de ano e de concentração de CO2 emissions_br &lt;- emissions_br[c(3,4)] # Mudando o nome das linhas para os anos da série temporal year &lt;- emissions_br[,1] row.names(emissions_br) &lt;- year # Criando tabela somente com as concentrações de CO2 como coluna emissions_br &lt;- emissions_br[c(2)] A partir disso, realiza-se a análise exploratória da base de dados: # EDA str(emissions_br) ## &#39;data.frame&#39;: 120 obs. of 1 variable: ## $ co2: num 2.1 2.51 2.44 2.62 2.8 ... head(emissions_br) ## co2 ## 1901 2.103 ## 1902 2.506 ## 1903 2.440 ## 1904 2.620 ## 1905 2.799 ## 1906 3.206 summary(emissions_br) ## co2 ## Min. : 2.103 ## 1st Qu.: 5.513 ## Median : 47.998 ## Mean :135.321 ## 3rd Qu.:221.342 ## Max. :557.901 Agora, é necessário transformar a base de dados em uma série temporal. Isso será realizado a partir da função ts(). # Criando a série temporal (ts) emissions_br.ts &lt;- ts(emissions_br$co2,start=1901, end=2020, frequency=1) Criada a série temporal, será realizada a estapa de Identificação utilizando a análise gráfica para obter mais conhecimento acerca da série. Portanto, iremos plotar a série temporal: # Análise gráfica # Tendência de aumento plot.ts(emissions_br.ts, ylab=&quot;Emissão de CO2&quot;, xlab=&quot;Anos&quot;) # Comentários: Tendência de aumento clara, não aparenta possuir sazonalidade já que não existem padrões que se repetem ao longo dos anos Conforme apresentado no gráfico, foi possível verificar que a série apresenta uma tendência de aumento clara. Além disso, não possui sazonalidade, já que não existem padrões que se repetem ao longo do ano. Sabe-se que uma série temporal é estacionária quando a sua média, variância e autocorrelação são contantes ao longo do tempo. Ou seja, uma série é estacionária quando não é dependente do tempo e não tem uma tendência ou sazonalidade. Como já foi verificado graficamente que a série possui uma tendência de aumento ao longo do tempo, é necessário verificar a partir de um teste estatístico se a série é estacionária. O teste que será utilizado é o Teste de Dickey-Fuller Aumentado (ADF). A hipótese nula do teste é que a série temporal possui uma raiz única, ou seja, não é estacionária. Portanto, se o p-value do teste for menor que o nível de significância \\(\\alpha\\) (5% ou 1%), a hipótese nula é rejeitada. Caso o p-value seja maior, a hipótese nula não é rejeitada e a série é não-estacionária. adf.drift &lt;- urca::ur.df(y=emissions_br.ts, type= &quot;drift&quot;, lag=24, selectlags=&quot;AIC&quot;) # Utiliza-se &quot;drift&quot; como argumento quando a série temporal possui tendência # Estatística de teste: Estacionariedade adf.drift@teststat ## tau2 phi1 ## statistic 1.765614 3.819948 adf.drift@cval #valores tabulados por MacKinnon (1996) ## 1pct 5pct 10pct ## tau2 -3.46 -2.88 -2.57 ## phi1 6.52 4.63 3.81 adf.test(emissions_br.ts, alternative=&quot;stationary&quot;, k=0) ## ## Augmented Dickey-Fuller Test ## ## data: emissions_br.ts ## Dickey-Fuller = -1.2795, Lag order = 0, p-value = 0.8758 ## alternative hypothesis: stationary A partir dos valores do teste ADF, é possível afirmar que a estatística teste (1,765614) é maior do que o valor máximo associado ao nível de confiança (-2,88). Dessa forma, conclui-se que a série não é estacionária. Outra função que pode ser usada para o teste ADF é a adf.test() do pacote tseries. O teste também mostra que a série não é estacionária, já que o p-value é maior que o nível de significância de 5%. Portanto, para transformá-la em estacionária, será realizada uma diferenciação: ts.plot(diff(emissions_br.ts, differences = 1)) # Com uma diferenciação é possível verificar que a série está estacionária na média. A partir da função adf.test será verificado se a série será estacionária com uma diferenciação: # Conferindo se a série realmente é estacionária # H0: não é estacionária # H1: é estacionária adf.test(diff(emissions_br.ts, differences = 1), alternative=&quot;stationary&quot;, k=0) ## Warning in adf.test(diff(emissions_br.ts, differences = 1), alternative = ## &quot;stationary&quot;, : p-value smaller than printed p-value ## ## Augmented Dickey-Fuller Test ## ## data: diff(emissions_br.ts, differences = 1) ## Dickey-Fuller = -7.9529, Lag order = 0, p-value = 0.01 ## alternative hypothesis: stationary Como o p-value apresenta o valor de 0,01, é possível afirmar que, a um intervalo de confiança de 95%, a série temporal é estacionária. Agora que a série é estacionária, seguiremos para a próxima fase (Identificação). # Identificação #FAC BETS::corrgram(diff(emissions_br.ts), lag.max = 36, style = &quot;normal&quot;) # Última observação significativa: 2 #FACP BETS::corrgram(diff(emissions_br.ts), type = &quot;partial&quot;, lag.max = 36, style = &quot;normal&quot;) # Última observação significativa: 1 ou 2 A partir disso, verifica-se que a Função de Autocorrelação, que determina a ordem das médias móveis (q), indica que a última observação significativa é a 2. E a Função de Autocorrelação Parcial, que determina a ordem p da componente autoregressiva, indica que a última observação significativa é a 1 ou 2. Dessa forma, o modelo que seria utilizado seria o ARIMA (2,1,2) ou (1,1,2). Para verificar se o modelo escolhido ARIMA (2,2,1) é o melhor fit, iremos utilizar a função auto.arima(). auto.arima(emissions_br.ts) ## Series: emissions_br.ts ## ARIMA(2,2,1) ## ## Coefficients: ## ar1 ar2 ma1 ## 0.2411 0.1609 -0.9677 ## s.e. 0.0976 0.0968 0.0300 ## ## sigma^2 = 101.8: log likelihood = -439.64 ## AIC=887.27 AICc=887.63 BIC=898.36 Como a FAC e FACP não foram claras, na fase de Estimação, para verificar se o modelo escolhido apresenta os critérios AIC e BIC minimizados, utilizou-se a função auto.arima() que retorna o modelo de melhor ajuste à série temporal de estudo. Como a função retornou o modelo ARIMA (2,2,1), este será utilizado, já que é o que é o modelo de melhor ajuste e, portanto, apresenta os critérios de informação minimizados. fit_co2 &lt;- arima(emissions_br.ts, c(2,2,1)) O próximo passo é verificar os resíduos do modelo: diag &lt;- tsdiag(fit_co2, gof.lag = 20) A partir do gráfico Standardized Residuals, visualmente os dados aparentam ter homocedasticidade e média zero, o que indica normalidade. Além disso, é possível verificar a existência de outliers, já que observações fora do intervalo [-3;3] seriam observações atípicas. Portanto, aparentemente existem alguns outliers a partir do ano 2000. A FAC dos resíduos (segundo gráfico) mostra que não há nenhuma defasagem significativa, portanto, o modelo ARIMA (2,2,1) está representando bem a série temporal. O gráfico p-values for Ljung-Box statistic não é confiável, já que os p-values são calculados sem levar em conta o fato de os resíduos terem sido gerados a partir de um modelo ajustado. Por mais que a conclusão deste gráfico mostre que não há dependência linear entre os resíduos, a informação não é confiável. O teste de Ljung &amp; Box será realizado para testar a autocorrelação dos resíduos: Box.test(fit_co2$residuals, lag=24, type=&quot;Ljung-Box&quot;, fitdf = 2) ## ## Box-Ljung test ## ## data: fit_co2$residuals ## X-squared = 20.466, df = 22, p-value = 0.5539 O teste mostra a ausência de autocorrelação linear nos resíduos, já que o p-value é maior que 0.05. Portanto, a hipótese nula de que a série temporal não possui autocorrelação até o lag 24 não pode ser rejeitada. A seguir será testada a estacionariedade da variância e, para isso, o teste Multiplicador de Lagrange para heterocedasticidade condicional autoregressiva será utilizado. Além disso, também será testada a normalidade dos resíduos com o teste de Shapiro-Wilk. # Variância arch.test(fit_co2) ## ARCH heteroscedasticity test for residuals ## alternative: heteroscedastic ## ## Portmanteau-Q test: ## order PQ p.value ## [1,] 4 32.7 1.40e-06 ## [2,] 8 85.0 4.66e-15 ## [3,] 12 85.7 3.39e-13 ## [4,] 16 85.8 1.46e-11 ## [5,] 20 85.9 3.89e-10 ## [6,] 24 85.9 6.75e-09 ## Lagrange-Multiplier test: ## order LM p.value ## [1,] 4 59.504 7.50e-13 ## [2,] 8 5.908 5.51e-01 ## [3,] 12 2.989 9.91e-01 ## [4,] 16 1.792 1.00e+00 ## [5,] 20 1.259 1.00e+00 ## [6,] 24 0.801 1.00e+00 #Normalidade dos resíduos shapiro.test(fit_co2$residuals) ## ## Shapiro-Wilk normality test ## ## data: fit_co2$residuals ## W = 0.71345, p-value = 5.383e-14 Em relação à variância, com p-value menor que 0.05, a hipótese nula de homocedasticidade da variância é rejeitada. No caso do teste de Shapiro-Wilk, verifica-se que como p-value é menor que 0.05, a hipótese nula de normalidade é rejeitada. Portanto, é possível verificar que os resíduos do modelo não estão adequados. Após a análise dos resíduos, mesmo utilizando o modelo que minimiza os critérios de informação (AIC e BIC) e apresenta ausência de autocorrelação linear nos resíduos, é possível afirmar que o modelo escolhido não é adequado para representar a série temporal devido a heterocedasticidade e não-normalidade dos resíduos. É importante que a análise de resíduos seja realizada sempre, já que, se não tivesse sido feita na aplicação, o modelo pareceria adequado. Apesar dos problemas da análise de resíduos, a etapa de Previsão será realizada para fins didáticos: # Estimação library(forecast) forecast_co2 &lt;- forecast::forecast(object=fit_co2, h=12) plot(forecast_co2) Dessa forma, a partir da função accuracy() é possível analisar se o modelo é adequado: # Métricas accuracy(fit_co2) ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 0.7565773 9.877258 4.93605 0.7380682 7.166957 0.748034 -0.01481926 Pelas métricas, o MAPE (Mean Absolute Percentage Error) demonstra que o erro de previsão está em 7,167%, o que, teoricamente, é um bom ajuste. Apesar da métrica adequada, como já analisamos, os resíduos do modelo não estão adequados e, dessa forma, é possível afirmar que o modelo também não está. O problema da heterocedasticidade pode ser resolvido a partir da inclusão de mais variáveis no modelo. A não-normalidade dos resíduos pode ser resolvida a partir de uma transformação logarítmica ou uma transformação de Box-Cox. "],["aplicação-2.html", "Aplicação 2", " Aplicação 2 Para a aplicação, será realizado o exemplo do capítulo 4 do livro de Barros, D., and D. (2017). Será utilizada como database a série temporal de vendas de passagens aéreas nos EUA de janeiro de 1949 a dezembro de 1960. # Intalando pacotes necessários packages &lt;- c(&quot;BETS&quot;,&quot;urca&quot;,&quot;TSA&quot;,&quot;forecast&quot;,&quot;lmtest&quot;,&quot;normtest&quot;,&quot;FinTS&quot;,&quot;xlsx&quot;) # Checando se os pacotes já estão instalados is.installed &lt;- function(mypkg){ is.element(mypkg, installed.packages()[,1]) } for(packages in packages){ if(!is.installed(packages)){ install.packages((packages), repos = &quot;http://cran.us.r-project.org&quot;) } } # Carregando a série temporal data(AirPassengers) # Análise gráfica ts.plot(AirPassengers, ylab = &quot;Vendas de Passagens &quot;, xlab = &quot;Anos&quot;) A partir do gráfico é possível perceber que existe uma tendência de aumento nas vendas de passagens. Em relação à variância, verifica-se que a distância entre os meses de maior e menor venda está aumentando, indicando uma variância não constante. Além de haver oscilações que se repetem anualmente, dando indícios de presença de sazonalidade, que será analisada mensalmente abaixo: monthplot(AirPassengers,ylab = &quot;Vendas de Passagens &quot;, xlab = &quot;Meses&quot;) No gráfico acima, os traços horizontais representam a média, a qual aumenta nos meses de férias dos EUA (Junho, Julho e Agosto). Pelos traços verticais e sua inclinação positiva, é possível verificar um aumento constante na venda de passagens ao longo dos anos. Ambos trazem indícios da não estacionariedade da série temporal. Conforma comentado ao longo do capítulo, a série temporal é composta por quatro componentes não observáveis e estas serão analisadas a seguir com a função decompose(). plot(decompose(AirPassengers)) A partir da figura acima, verifica-se que a série temporal é fortemente afetada pela tendência (trend), além da sazonalidade (seasonal). Sobra, ainda, a componente aleatória que é levemente “contaminada” pela componente sazonal, como é possível verificar na comparação entre random e seasonal no gráfico. Conforme citado anteriormente, as análises gráficas dão indícios de não estacionariedade. Portanto, a estacionariedade será testada com significância estatística nas partes não sazonal e sazonal da série temporal. Para isso, serão realizados os seguintes passos: 1) análise gráfica; 2) análise da média e variância em diferentes períodos de tempo; 3) análise da Função de Autocorrelação (FAC) 4) testes de raiz unitária. Parte não sazonal A identificação da autocorrelação entre o valor atual e suas defasagens (lags) é feita abaixo. O intervalo de confiança (de 95%) são as linhas pontilhadas vermelhas e valores acima ou abaixo da linha são estatisticamente significantes. Portanto, existe autocorrelação significativa até o lag 36. require(BETS) ## Loading required package: BETS ## Warning: package &#39;BETS&#39; was built under R version 4.2.3 ## ## Attaching package: &#39;BETS&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## predict BETS::corrgram(AirPassengers, lag.max = 36, ci=0.95) Existem três formas de realizar o teste ADF: raiz unitária + constante + tendência determinística escolhe-se ‘type=“trend”’; raiz unitária + constante, type = \"dryft\"; raiz unitária, type= \"none\". No teste de raiz unitária, a hipótese nula é que a série temporal possui uma raiz unitária (ST é não estacionária) e a hipótese alternativa é que a série é estacionária. Considerando a série temporal como sem tendência, com variância constante e com o critério de informação sendo o AIC: adf.drift &lt;- urca::ur.df(y=AirPassengers, type= &quot;drift&quot;, lag=24, selectlags=&quot;AIC&quot;) BETS::corrgram(adf.drift@res, lag.max=36) # Estatística de teste adf.drift@teststat ## tau2 phi1 ## statistic 1.85818 7.914366 adf.drift@cval #valores tabulados por MacKinnon (1996) ## 1pct 5pct 10pct ## tau2 -3.46 -2.88 -2.57 ## phi1 6.52 4.63 3.81 summary(adf.drift) ## Length Class Mode ## 1 ur.df S4 A partir do gráfico, é possível afirmar que a estatística teste (\\(\\tau^2=1,8582\\)) é maior do que o valor máximo associado ao nível de confiança (-2,88). Portanto, a hipótese nula não é rejeitada e é possível concluir que a série temporal não é estacionária e, portanto, possui raiz unitária. Agora precisamos descobrir o número de diferenciações necessárias para torná-la estacionária. Inicialmente, será realizada uma diferenciação para verificar se torna a série estacionária: ts.plot(diff(AirPassengers, lag=1, differences=1)) BETS::corrgram(diff(AirPassengers, lag=1, differences=1), lag.max=36) Com uma diferenciação é possível verificar que a série está estacionária na média. Porém, a ST está crescendo ao longo do tempo e, dessa forma, sua variância não está constante. Uma estratégia importante para tornar a variância constante, é aplicar log na série temporal. É o que será feito: ts.plot(diff(log(AirPassengers), lag=1, differences=1)) BETS::corrgram(diff(log(AirPassengers), lag=1, differences=1), lag.max=48) Agora, a parte não sazonal da série temporal é estacionária. Parte sazonal Pela FAC acima, é possível verificar que nos lags sazonais (12, 24, 36…) a função de autocorrelação apresenta um decrescimento, o que indica que a série temporal não é estacionária na parte sazonal. Da mesma forma realizada na parte não sazonal, também será necessário realizar uma diferenciação para verificar se ela se torna estacionária. Nesse caso, o lag é modificado para 12 e, assim, é possível realizar a diferenciação na parte sazonal: BETS::corrgram(diff(diff(log(AirPassengers), lag = 1, differences = 1), lag = 12, differences = 1), lag.max = 48) A partir da figura acima, é possível verificar que a FAC não apresenta mais o decrescimento de antes, além de ter cortes bruscos nos lags 1 e 12. Para verificar a estacionariedade, será realizado o teste de RU: adf.drift2 &lt;- urca::ur.df(y = diff(diff(log(AirPassengers), lag = 1), lag = 12), type = &quot;drift&quot;, lags = 24, selectlags = &quot;AIC&quot;) adf.drift2@teststat # Estatística de Teste ## tau2 phi1 ## statistic -4.039891 8.160779 adf.drift2@cval #valores tabulados por MacKinnon (1996) ## 1pct 5pct 10pct ## tau2 -3.46 -2.88 -2.57 ## phi1 6.52 4.63 3.81 BETS::corrgram(adf.drift2@res, lag.max = 36) A estatística teste (\\(\\tau^2=-4.03891\\)) é menor do que o valor máximo associado ao nível de confiança (-2,88). Portanto, conclui-se que a série é estacionária! A partir daqui, será aplicado o método de Box &amp; Jenkins. Para a fase de identificação será observado a FAC (Função de Autocorrelação) e FACP (Função de Autocorrelação Parcial) do modelo estacionário: #FAC BETS::corrgram(diff(diff(log(AirPassengers), lag = 1, differences = 1), lag = 12, differences = 1), lag.max = 48) #FACP BETS::corrgram(diff(diff(log(AirPassengers), lag = 1, differences = 1), lag = 12, differences = 1), type = &quot;partial&quot;, lag.max = 48) É importante citar que o próximo passo é verificar as FAC e FACP e obter “p” e “q” nos lags 1, 2, 3… (Não-sazonal) e lags 12, 24, 36… (Sazonais). Em um primeiro cenário, como a FAC determina a ordem das médias móveis (q), é possível verificar que a última observação significativa da FAC é o lag 1, portanto, q=1. Já a FACP dá a ordem da componente autoregressiva (p). Da mesma forma que na FAC, a última observação sigificativa é a 1. Como foi realizada uma diferenciação na parte sazonal e uma na parte não sazonal, a ordem de integração é 1 (d=1). O modelo resultante seria SARIMA(1,1,1)(1,1,1). Em um segundo cenário, a última observação significativa da FAC continua sendo o lag 1 (q=1). A ordem de integração também continua a mesma. Mas no caso da FACP, é possível visualizar um decrescimento não só na parte sazonal, como também na parte não sazonal (p=0). Portanto, o modelo seria SARIMA(0,1,1)(0,1,1). Com esses possíveis modelos, podemos iniciar a etapa de Estimação. Será utilizada a função Arima(), em que a variável de entrada é a série temporal original. A transformação logarítmica necessária pra deixar a variância constante é garantida pelo argumento lambda = 0. A função Arima() automaticamente já diferencia a série. library(&quot;forecast&quot;) fit.air &lt;- Arima(AirPassengers, order = c(1,1,1), seasonal = c(1,1,1), method = &quot;ML&quot;, lambda = 0) fit.air ## Series: AirPassengers ## ARIMA(1,1,1)(1,1,1)[12] ## Box Cox transformation: lambda= 0 ## ## Coefficients: ## ar1 ma1 sar1 sma1 ## 0.1668 -0.5616 -0.0994 -0.497 ## s.e. 0.2458 0.2114 0.1540 0.136 ## ## sigma^2 = 0.00138: log likelihood = 245.16 ## AIC=-480.31 AICc=-479.83 BIC=-465.93 O próximo passo é verificar se os parâmetros do modelo são significativos: BETS::t_test(fit.air) ## Coeffs Std.Errors t Crit.Values Rej.H0 ## ar1 0.16679124 0.2457980 0.6785705 1.977304 FALSE ## ma1 -0.56163441 0.2114211 2.6564723 1.977304 TRUE ## sar1 -0.09938487 0.1539918 0.6453907 1.977304 FALSE ## sma1 -0.49700743 0.1360485 3.6531644 1.977304 TRUE Como o parâmetro AR (ar1) foi rejeitado porque não é sigificativo, é importante retirar o parâmetro e analisá-lo novamente. Ele será retirado colocando o parâmetro p=0. fit.air &lt;- Arima(AirPassengers, order=c(0,1,1), seasonal = c(0,1,1), method = &quot;ML&quot;, lambda = 0) fit.air ## Series: AirPassengers ## ARIMA(0,1,1)(0,1,1)[12] ## Box Cox transformation: lambda= 0 ## ## Coefficients: ## ma1 sma1 ## -0.4018 -0.5569 ## s.e. 0.0896 0.0731 ## ## sigma^2 = 0.001371: log likelihood = 244.7 ## AIC=-483.4 AICc=-483.21 BIC=-474.77 BETS::t_test(fit.air) ## Coeffs Std.Errors t Crit.Values Rej.H0 ## ma1 -0.4018268 0.08964405 4.482470 1.977054 TRUE ## sma1 -0.5569466 0.07309948 7.619023 1.977054 TRUE Como todos os parâmetros do modelo \\(SARIMA(0,1,1)(0,1,1)_{12}\\) são significativos e os critérios de informação AIC, AICc e BIC foram minimizados, esse será o modelo escolhido para a próxima fase: o diagnóstico. No diagnóstico, os resíduos serão testados, assim como foram testados no capítulo anterior. Serão analisados a ausência de autocorrelação linear, de heterocedasticidade condicional e normalidade. Para uma análise geral, será utilizada a função tsdiag(). diag &lt;- tsdiag(fit.air, gof.lag = 20) A partir do gráfico Standarized Residuals, visualmente os dados aparentam ter heterocedasticidade e média zero, o que indica normalidade. Além disso, é possível verificar a existência de outliers, já que observações fora do intervalo [-3;3] seriam observações atípicas. Portanto, aparentemente não existem outliers. A FAC dos resíduos (segundo gráfico) mostra que não há nenhuma defasagem significativa, portanto, o modelo SARIMA está representando bem a série temporal. O gráfico p-values for Ljung-Box statistic não é confiável, já que os p-values são calculados sem levar em conta o fato de os resíduos terem sido gerados a partir de um modelo ajustado. Por mais que a conclusão deste gráfico mostre que não há dependência linear entre os resíduos, a informação não é confiável. O teste de Ljung &amp; Box será realizado para testar a autocorrelação dos resíduos: Box.test(fit.air$residuals, lag=24, type=&quot;Ljung-Box&quot;, fitdf = 2) ## ## Box-Ljung test ## ## data: fit.air$residuals ## X-squared = 26.446, df = 22, p-value = 0.233 O teste mostra a ausência de autocorrelação linear nos resíduos, já que o p-value é maior que 0.05. Portanto, a hipótese nula de que a série temporal não possui autocorrelação até o lag 24 não pode ser rejeitada. A seguir será testada a estacionariedade da variância e, para isso, o teste Multiplicador de Lagrange para heterocedasticidade condicional autoregressiva será utilizado. Além disso, também será testada a normalidade dos resíduos com o teste de Shapiro-Wilk. # Variância FinTS::ArchTest(fit.air$residuals, lags=12) ## ## ARCH LM-test; Null hypothesis: no ARCH effects ## ## data: fit.air$residuals ## Chi-squared = 14.859, df = 12, p-value = 0.2493 #Normalidade dos resíduos shapiro.test(fit.air$residuals) ## ## Shapiro-Wilk normality test ## ## data: fit.air$residuals ## W = 0.98637, p-value = 0.1674 Em relação à variância, com p-value maior que 0.05, a hipótese nula de estacionariedade da vaiância nao é rejeitada. No caso do teste de Shapiro-Wilk, verifica-se que como p-value é maior que 0.05, a hipótese nula de normalidade também não é rejeitada. A próxima fase será a Previsão utilizando o pacote forecast(). O argumento “h” é o horizonte de previsão e o “level” é o nível de confiança. Para verificar se a previsão é adequada, serão analisadas as métricas utilizando a função accuracy. library(forecast) # Previsão forecast_air &lt;- forecast::forecast(object=fit.air, h=12) plot(forecast_air) # Métricas accuracy(fit.air) ## ME RMSE MAE MPE MAPE MASE ## Training set 0.05140376 10.15504 7.357555 -0.004079321 2.623637 0.2297061 ## ACF1 ## Training set -0.03689736 Pelas métricas, o MAPE demonstra que o erro de previsão está em 2,62%, o que é muito bom. Dessa forma, conseguimos confirmar que a previsão está adequada! References "],["material-complementar-1.html", "Material Complementar", " Material Complementar Introdução à Análise de Séries Temporais (youtube) Séries Temporais (youtube) "]]
